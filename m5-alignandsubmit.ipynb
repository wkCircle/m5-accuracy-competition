{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Matthias Anderer\n",
    "\n",
    "Copyright for aggregation code snippets 2020 by user: https://www.kaggle.com/lebroschar (name unknown)\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall approach\n",
    "\n",
    "We have two different inputs: \n",
    "\n",
    "1) Bottom level forecasts on item level (30490 signal) that are derived from a lgbm model that models a probability of this item being bought based on datatime features, price features and a few other features that are not time dependent. (Credits: https://www.kaggle.com/kyakovlev/m5-simple-fe)\n",
    "2) Top level forecasts for the levels 1-5 that are created with N-Beats. \n",
    "\n",
    "We can now aggregate the bottom level \"probabilit draws\" up to the levels 1-5. By comparing/aligning the possible results we can select the most suitable probability distribution for the forecast period. ( The multiplier in the custom loss of the bottom level lgbm models seems to help adjust for trend or other effects not fully understood yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall analysis result: \n",
    "\n",
    "The multiplier 0.95 seems to represent the lowest available fit so we build an ensemble with the 2 upper and 2 lower distributions to generate a robust test loss.\n",
    "<br><br>\n",
    "Final-11: 0.9 <br>\n",
    "Final-12: 0.93 <br>\n",
    "Final-17: 0.95 <br>\n",
    "Final-13: 0.97 <br>\n",
    "Final-16: 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NBEATS reference predictions for global alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NBeats predictions trained and predicted on Colab with two different settings (only change in setting is num_epochs to get slightly different ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "nbeats_pred01_df = pd.read_csv('../input/m5alignnbeatsv01/nbeats_toplvl_forecasts1.csv')\n",
    "nbeats_pred02_df = pd.read_csv('../input/m5alignnbeatsv02/nbeats_toplvl_forecasts2.csv')\n",
    "\n",
    "#nbeats_pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load bottom level lgb predictions for alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUILD_ENSEMBLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_ENSEMBLE:\n",
    "    \n",
    "    pred_01_df = pd.read_csv('../input/m5-final-13/submission_v1.csv')\n",
    "    pred_02_df = pd.read_csv('../input/fork-of-m5-final-11/submission_v1.csv')\n",
    "    pred_03_df = pd.read_csv('../input/m5-final-12/submission_v1.csv')\n",
    "    pred_04_df = pd.read_csv('../input/m5-final-17/submission_v1.csv')\n",
    "    pred_05_df = pd.read_csv('../input/m5-final-16/submission_v1.csv')\n",
    "    #pred_06_df = pd.read_csv('..')\n",
    "\n",
    "    avg_pred = ( np.array(pred_01_df.values[:,1:]) \n",
    "                + np.array(pred_02_df.values[:,1:]) \n",
    "                + np.array(pred_03_df.values[:,1:])\n",
    "                + np.array(pred_04_df.values[:,1:])  \n",
    "                + np.array(pred_05_df.values[:,1:])  \n",
    "               # + np.array(pred_06_df.values[:,1:])  \n",
    "               ) /5.0\n",
    "    \n",
    "    ## Loading predictions\n",
    "    valid_pred_df = pd.DataFrame(avg_pred, columns=pred_01_df.columns[1:])\n",
    "    submission_pred_df = pd.concat([pred_01_df['id'],valid_pred_df],axis=1)\n",
    "    \n",
    "else:\n",
    "    print('Should not submit single distibution')\n",
    "    #submission_pred_df = pd.read_csv('../input/m5-final-13/submission_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill validation rows - we have no info about validation scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it would not make sense at all to score public validation data it might be safest to set the submission validation values to the ground truth....\n",
    "\n",
    "Spamming the LB a bit more ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>d_1914</th>\n",
       "      <th>d_1915</th>\n",
       "      <th>d_1916</th>\n",
       "      <th>d_1917</th>\n",
       "      <th>d_1918</th>\n",
       "      <th>d_1919</th>\n",
       "      <th>d_1920</th>\n",
       "      <th>d_1921</th>\n",
       "      <th>d_1922</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30485</th>\n",
       "      <td>FOODS_3_823_WI_3_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30486</th>\n",
       "      <td>FOODS_3_824_WI_3_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30487</th>\n",
       "      <td>FOODS_3_825_WI_3_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30488</th>\n",
       "      <td>FOODS_3_826_WI_3_validation</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30489</th>\n",
       "      <td>FOODS_3_827_WI_3_validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30490 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  id  d_1914  d_1915  d_1916  d_1917  d_1918  \\\n",
       "0      HOBBIES_1_001_CA_1_validation       0       0       0       2       0   \n",
       "1      HOBBIES_1_002_CA_1_validation       0       1       0       0       0   \n",
       "2      HOBBIES_1_003_CA_1_validation       0       0       1       1       0   \n",
       "3      HOBBIES_1_004_CA_1_validation       0       0       1       2       4   \n",
       "4      HOBBIES_1_005_CA_1_validation       1       0       2       3       1   \n",
       "...                              ...     ...     ...     ...     ...     ...   \n",
       "30485    FOODS_3_823_WI_3_validation       0       0       0       2       2   \n",
       "30486    FOODS_3_824_WI_3_validation       0       1       1       1       0   \n",
       "30487    FOODS_3_825_WI_3_validation       0       0       1       1       0   \n",
       "30488    FOODS_3_826_WI_3_validation       1       3       0       1       2   \n",
       "30489    FOODS_3_827_WI_3_validation       0       0       0       0       0   \n",
       "\n",
       "       d_1919  d_1920  d_1921  d_1922  ...  d_1932  d_1933  d_1934  d_1935  \\\n",
       "0           3       5       0       0  ...       2       4       0       0   \n",
       "1           0       0       0       0  ...       0       1       2       1   \n",
       "2           2       1       0       0  ...       1       0       2       0   \n",
       "3           1       6       4       0  ...       1       1       0       4   \n",
       "4           0       3       2       3  ...       0       0       0       2   \n",
       "...       ...     ...     ...     ...  ...     ...     ...     ...     ...   \n",
       "30485       0       0       0       2  ...       1       0       3       0   \n",
       "30486       0       0       0       1  ...       0       0       0       0   \n",
       "30487       2       1       1       0  ...       0       0       1       2   \n",
       "30488       1       0       2       1  ...       1       1       1       4   \n",
       "30489       1       1       1       2  ...       1       2       0       5   \n",
       "\n",
       "       d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0           0       0       3       3       0       1  \n",
       "1           1       0       0       0       0       0  \n",
       "2           0       0       2       3       0       1  \n",
       "3           0       1       3       0       2       6  \n",
       "4           1       0       0       2       1       0  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "30485       1       1       0       0       1       1  \n",
       "30486       0       0       1       0       1       0  \n",
       "30487       0       1       0       1       0       2  \n",
       "30488       6       0       1       1       1       0  \n",
       "30489       4       0       2       2       5       1  \n",
       "\n",
       "[30490 rows x 29 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_gt_data = pd.read_csv('input/data/sales_train_evaluation.csv')\n",
    "validation_gt_data['id'] = validation_gt_data['id'].str.replace('_evaluation','_validation')\n",
    "validation_gt_data = validation_gt_data.drop(['item_id','dept_id','cat_id','store_id','state_id'],axis=1)\n",
    "validation_gt_data = pd.concat([validation_gt_data[['id']],validation_gt_data.iloc[:,-28:]],axis=1)\n",
    "# validation_gt_data.columns=submission_pred_df.columns.values\n",
    "validation_gt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pred_df = pd.concat([validation_gt_data, submission_pred_df.iloc[30490:,:]],axis=0).reset_index(drop=True)\n",
    "submission_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only work on evaluation forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_lvl_pred_df = submission_pred_df.iloc[30490:,:].reset_index(drop=True)\n",
    "bottom_lvl_pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct level descriptions for aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = bottom_lvl_pred_df.id.str.split(pat='_',expand=True)\n",
    "name_cols['dept_id']=name_cols[0]+'_'+name_cols[1]\n",
    "name_cols['store_id']=name_cols[3]+'_'+name_cols[4]\n",
    "name_cols = name_cols.rename(columns={0: \"cat_id\", 3: \"state_id\"})\n",
    "name_cols = name_cols.drop([1,2,4,5],axis=1)\n",
    "bottom_lvl_pred_df = pd.concat([name_cols,bottom_lvl_pred_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build aggregates of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column groups\n",
    "cat_cols = ['id', 'dept_id', 'cat_id',  'store_id', 'state_id']\n",
    "ts_cols = [col for col in bottom_lvl_pred_df.columns if col not in cat_cols]\n",
    "ts_dict = {t: int(t[1:]) for t in ts_cols}\n",
    "\n",
    "# Describe data\n",
    "print('  unique forecasts: %i' % bottom_lvl_pred_df.shape[0])\n",
    "for col in cat_cols:\n",
    "    print('   N_unique %s: %i' % (col, bottom_lvl_pred_df[col].nunique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. All products, all stores, all states (1 series)\n",
    "all_sales = pd.DataFrame(bottom_lvl_pred_df[ts_cols].sum()).transpose()\n",
    "all_sales['id_str'] = 'all'\n",
    "all_sales = all_sales[ ['id_str'] +  [c for c in all_sales if c not in ['id_str']] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. All products by state (3 series)\n",
    "state_sales = bottom_lvl_pred_df.groupby('state_id',as_index=False)[ts_cols].sum()\n",
    "state_sales['id_str'] = state_sales['state_id'] \n",
    "state_sales = state_sales[ ['id_str'] +  [c for c in state_sales if c not in ['id_str']] ]\n",
    "state_sales = state_sales.drop(['state_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. All products by store (10 series)\n",
    "store_sales = bottom_lvl_pred_df.groupby('store_id',as_index=False)[ts_cols].sum()\n",
    "store_sales['id_str'] = store_sales['store_id'] \n",
    "store_sales = store_sales[ ['id_str'] +  [c for c in store_sales if c not in ['id_str']] ]\n",
    "store_sales = store_sales.drop(['store_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. All products by category (3 series)\n",
    "cat_sales = bottom_lvl_pred_df.groupby('cat_id',as_index=False)[ts_cols].sum()\n",
    "cat_sales['id_str'] = cat_sales['cat_id'] \n",
    "cat_sales = cat_sales[ ['id_str'] +  [c for c in cat_sales if c not in ['id_str']] ]\n",
    "cat_sales = cat_sales.drop(['cat_id'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. All products by department (7 series)\n",
    "dept_sales = bottom_lvl_pred_df.groupby('dept_id',as_index=False)[ts_cols].sum()\n",
    "dept_sales['id_str'] = dept_sales['dept_id'] \n",
    "dept_sales = dept_sales[ ['id_str'] +  [c for c in dept_sales if c not in ['id_str']] ]\n",
    "dept_sales = dept_sales.drop(['dept_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_agg = pd.concat([all_sales,state_sales,store_sales,cat_sales,dept_sales],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbeats_pred01_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating comparision metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "If prediction is bigger than \"true\" values error will be positive -> prediction is overshooting (pos error)\n",
    "\n",
    "If prediction is smaller than \"true\" values error will be negative -> prediction is undershooting (neg error) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBeats 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = nbeats_pred01_df[['id_str']]\n",
    "\n",
    "## Calculate errors\n",
    "## CAUTION: nbeats_pred_df is \"truth\"/actual values in this context\n",
    "error = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred01_df.values[:,1:]) ) \n",
    "\n",
    "## Calc RMSSE\n",
    "successive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\n",
    "denom = successive_diff.mean(1)\n",
    "\n",
    "num = error.mean(1)**2\n",
    "rmsse = num / denom\n",
    "\n",
    "metrics_df['rmsse'] = rmsse\n",
    "\n",
    "## Not so clean Pandas action :-) - supressing warnings for now...\n",
    "metrics_df['mean_error'] = error.mean(1)\n",
    "metrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n",
    "\n",
    "squared_error = error **2\n",
    "mean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n",
    "\n",
    "metrics_df['rmse'] = np.sqrt( mean_squ_err )\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBeats 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = nbeats_pred02_df[['id_str']]\n",
    "\n",
    "## Calculate errors\n",
    "## CAUTION: nbeats_pred_df is \"truth\"/actual values in this context\n",
    "error = ( np.array(all_pred_agg.values[:,1:]) - np.array(nbeats_pred02_df.values[:,1:]) ) \n",
    "\n",
    "## Calc RMSSE\n",
    "successive_diff = np.diff(nbeats_pred01_df.values[:,1:]) ** 2\n",
    "denom = successive_diff.mean(1)\n",
    "\n",
    "num = error.mean(1)**2\n",
    "rmsse = num / denom\n",
    "\n",
    "metrics_df['rmsse'] = rmsse\n",
    "\n",
    "## Not so clean Pandas action :-) - supressing warnings for now...\n",
    "metrics_df['mean_error'] = error.mean(1)\n",
    "metrics_df['mean_abs_error'] = np.absolute(error).mean(1)\n",
    "\n",
    "squared_error = error **2\n",
    "mean_squ_err = np.array(squared_error.mean(1), dtype=np.float64) \n",
    "\n",
    "metrics_df['rmse'] = np.sqrt( mean_squ_err )\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBeats 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,nbeats_pred01_df.shape[0]):\n",
    "    plot_df = pd.concat( [nbeats_pred01_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n",
    "    plot_df = plot_df.iloc[1:,]\n",
    "    plot_df = plot_df.rename(columns={0:'NBeats',1:'Predictions'})\n",
    "    plot_df = plot_df.reset_index()\n",
    "    #plot_df\n",
    "    \n",
    "    plot_df.plot(x='index', y=['NBeats', 'Predictions'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NBeats 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,nbeats_pred02_df.shape[0]):\n",
    "    plot_df = pd.concat( [nbeats_pred02_df.iloc[i], all_pred_agg.iloc[i] ]  , axis=1, ignore_index=True)\n",
    "    plot_df = plot_df.iloc[1:,]\n",
    "    plot_df = plot_df.rename(columns={0:'NBeats',1:'Predictions'})\n",
    "    plot_df = plot_df.reset_index()\n",
    "    #plot_df\n",
    "    \n",
    "    plot_df.plot(x='index', y=['NBeats', 'Predictions'] ,figsize=(10,5), grid=True, title=nbeats_pred02_df.iloc[i,0]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit based on above analysis and manual selection/clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_pred_df.to_csv('m5-final-submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('poetry-modeling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9fdaeeb7f23b80181d796b81c382cf57b09494118cd09f4f1f2649b466761a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
