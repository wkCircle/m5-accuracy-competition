{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Konstantin Yakovlev, Matthias Anderer\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "import pathlib\n",
    "import lightgbm as lgb\n",
    "\n",
    "# custom imports\n",
    "from multiprocessing import Pool        # Multiprocess Runs\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helpers\n",
    "#################################################################################\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss func hyperparamter: LOSS_MUTIPLIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_MULTIPLIER = 1.0 # Set multiplier according to desired under-/overshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom loss function\n",
    "def custom_asymmetric_train(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    grad = np.where(residual < 0, -2 * residual, -2 * residual * LOSS_MULTIPLIER)\n",
    "    hess = np.where(residual < 0, 2, 2 * LOSS_MULTIPLIER)\n",
    "    return grad, hess\n",
    "\n",
    "# define custom evaluation metric\n",
    "def custom_asymmetric_valid(y_pred, y_true):\n",
    "    y_true = y_true.get_label()\n",
    "    residual = (y_true - y_pred).astype(\"float\")\n",
    "    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * LOSS_MULTIPLIER) \n",
    "    return \"custom_asymmetric_eval\", np.mean(loss), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Helper to load data by store ID\n",
    "#################################################################################\n",
    "# Read data\n",
    "def get_data_by_store(store):\n",
    "    \n",
    "    # Read and contact basic feature\n",
    "    df = pd.concat([pd.read_pickle(BASE),\n",
    "                    pd.read_pickle(PRICE).iloc[:,2:],\n",
    "                    pd.read_pickle(CALENDAR).iloc[:,2:]],\n",
    "                    axis=1)\n",
    "    \n",
    "    # Leave only relevant store\n",
    "    df = df[df['store_id']==store]\n",
    "    \n",
    "    ############\n",
    "    # Create features list\n",
    "    features = [col for col in list(df) if col not in remove_features]\n",
    "    df = df[['id','d',TARGET]+features]\n",
    "    \n",
    "    # Skipping first n rows\n",
    "    df = df[df['d']>=START_TRAIN].reset_index(drop=True)\n",
    "    \n",
    "    return df, features\n",
    "\n",
    "# Recombine Test set after training\n",
    "def get_base_test():\n",
    "    base_test = pd.DataFrame()\n",
    "\n",
    "    for store_id in STORES_IDS:\n",
    "        temp_df = pd.read_pickle(output_parent/f'test_{store_id}.pkl')\n",
    "        temp_df['store_id'] = store_id\n",
    "        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)\n",
    "    \n",
    "    return base_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Variables Def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Model params\n",
    "#################################################################################\n",
    "lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'tweedie',\n",
    "        'tweedie_variance_power': 1.1,\n",
    "        'metric':'rmse',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'learning_rate': 0.2,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 1, \n",
    "        'colsample_bytree': 0.85,\n",
    "        'colsample_bynode': 0.85,\n",
    "        #'min_data_per_leaf': 25,\n",
    "        #'num_leaves': 200,\n",
    "        'lambda_l1': 0.5,\n",
    "        'lambda_l2': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stores ids are: ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']\n"
     ]
    }
   ],
   "source": [
    "########################### Vars\n",
    "#################################################################################\n",
    "VER = 1                          # Our model version\n",
    "SEED = 42                        # We want all things\n",
    "seed_everything(SEED)            # to be as deterministic \n",
    "lgb_params['seed'] = SEED        # as possible\n",
    "N_CORES = psutil.cpu_count()     # Available CPU cores\n",
    "\n",
    "\n",
    "#LIMITS and const\n",
    "TARGET      = 'sales'            # Our target\n",
    "START_TRAIN = 0                  # We can skip some rows (Nans/faster training)\n",
    "END_TRAIN   = 1913+28            # End day of our train set\n",
    "P_HORIZON   = 28                 # Prediction horizon\n",
    "USE_AUX     = False               # Use or not pretrained models\n",
    "\n",
    "#FEATURES to remove\n",
    "## These features lead to overfit\n",
    "## or values not present in test set\n",
    "remove_features = ['id','state_id','store_id',\n",
    "                   'date','wm_yr_wk','d',TARGET]\n",
    "\n",
    "#PATHS for Features\n",
    "ORIGINAL = pathlib.Path(\"./input/data\")\n",
    "input_parent = pathlib.Path(\"./input/fe_out\")\n",
    "BASE     = input_parent/'grid_part_1.pkl'\n",
    "PRICE    = input_parent/'grid_part_2.pkl'\n",
    "CALENDAR = input_parent/'grid_part_3.pkl'\n",
    "output_parent = pathlib.Path(\"./input/bottom_out\")\n",
    "\n",
    "#STORES ids\n",
    "STORES_IDS = pd.read_csv(ORIGINAL/'sales_train_validation.csv')['store_id']\n",
    "STORES_IDS = list(STORES_IDS.unique())\n",
    "print(\"stores ids are:\", STORES_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train CA_1...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.487384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5643\n",
      "[LightGBM] [Info] Number of data points in the train set: 4751349, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.56557\tvalid_1's rmse: 2.20605\n",
      "[200]\ttraining's rmse: 2.4639\tvalid_1's rmse: 2.13062\n",
      "[300]\ttraining's rmse: 2.40439\tvalid_1's rmse: 2.10047\n",
      "[400]\ttraining's rmse: 2.36267\tvalid_1's rmse: 2.07737\n",
      "[500]\ttraining's rmse: 2.33112\tvalid_1's rmse: 2.05069\n",
      "[600]\ttraining's rmse: 2.30341\tvalid_1's rmse: 2.04221\n",
      "[700]\ttraining's rmse: 2.27722\tvalid_1's rmse: 2.02615\n",
      "[800]\ttraining's rmse: 2.25612\tvalid_1's rmse: 2.02238\n",
      "[900]\ttraining's rmse: 2.23541\tvalid_1's rmse: 2.00975\n",
      "[1000]\ttraining's rmse: 2.21714\tvalid_1's rmse: 1.99895\n",
      "[1100]\ttraining's rmse: 2.20066\tvalid_1's rmse: 1.97384\n",
      "[1200]\ttraining's rmse: 2.18526\tvalid_1's rmse: 1.9655\n",
      "[1300]\ttraining's rmse: 2.1715\tvalid_1's rmse: 1.96011\n",
      "[1400]\ttraining's rmse: 2.15872\tvalid_1's rmse: 1.94947\n",
      "[1500]\ttraining's rmse: 2.14682\tvalid_1's rmse: 1.94034\n",
      "[1600]\ttraining's rmse: 2.13565\tvalid_1's rmse: 1.93648\n",
      "[1700]\ttraining's rmse: 2.12556\tvalid_1's rmse: 1.9329\n",
      "[1800]\ttraining's rmse: 2.11485\tvalid_1's rmse: 1.92561\n",
      "[1900]\ttraining's rmse: 2.10488\tvalid_1's rmse: 1.92023\n",
      "[2000]\ttraining's rmse: 2.09608\tvalid_1's rmse: 1.91598\n",
      "[2100]\ttraining's rmse: 2.0873\tvalid_1's rmse: 1.91195\n",
      "[2200]\ttraining's rmse: 2.07885\tvalid_1's rmse: 1.9078\n",
      "[2300]\ttraining's rmse: 2.07006\tvalid_1's rmse: 1.89925\n",
      "[2400]\ttraining's rmse: 2.06212\tvalid_1's rmse: 1.89293\n",
      "[2500]\ttraining's rmse: 2.05453\tvalid_1's rmse: 1.88771\n",
      "[2600]\ttraining's rmse: 2.04664\tvalid_1's rmse: 1.88122\n",
      "[2700]\ttraining's rmse: 2.03947\tvalid_1's rmse: 1.87411\n",
      "[2800]\ttraining's rmse: 2.03252\tvalid_1's rmse: 1.86976\n",
      "[2900]\ttraining's rmse: 2.02548\tvalid_1's rmse: 1.86422\n",
      "[3000]\ttraining's rmse: 2.0185\tvalid_1's rmse: 1.85751\n",
      "[3100]\ttraining's rmse: 2.01152\tvalid_1's rmse: 1.85207\n",
      "[3200]\ttraining's rmse: 2.00542\tvalid_1's rmse: 1.84634\n",
      "[3300]\ttraining's rmse: 1.99929\tvalid_1's rmse: 1.8409\n",
      "[3400]\ttraining's rmse: 1.99371\tvalid_1's rmse: 1.83528\n",
      "[3500]\ttraining's rmse: 1.98813\tvalid_1's rmse: 1.83171\n",
      "[3600]\ttraining's rmse: 1.98257\tvalid_1's rmse: 1.82566\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.98257\tvalid_1's rmse: 1.82566\n",
      "Train CA_2...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.571452 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5578\n",
      "[LightGBM] [Info] Number of data points in the train set: 4329697, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.12787\tvalid_1's rmse: 2.02454\n",
      "[200]\ttraining's rmse: 2.07305\tvalid_1's rmse: 1.98646\n",
      "[300]\ttraining's rmse: 2.03202\tvalid_1's rmse: 1.96068\n",
      "[400]\ttraining's rmse: 2.00377\tvalid_1's rmse: 1.93308\n",
      "[500]\ttraining's rmse: 1.98291\tvalid_1's rmse: 1.91434\n",
      "[600]\ttraining's rmse: 1.9649\tvalid_1's rmse: 1.89145\n",
      "[700]\ttraining's rmse: 1.94913\tvalid_1's rmse: 1.88237\n",
      "[800]\ttraining's rmse: 1.93335\tvalid_1's rmse: 1.8691\n",
      "[900]\ttraining's rmse: 1.91991\tvalid_1's rmse: 1.86123\n",
      "[1000]\ttraining's rmse: 1.90846\tvalid_1's rmse: 1.85332\n",
      "[1100]\ttraining's rmse: 1.89827\tvalid_1's rmse: 1.84365\n",
      "[1200]\ttraining's rmse: 1.8881\tvalid_1's rmse: 1.83786\n",
      "[1300]\ttraining's rmse: 1.87791\tvalid_1's rmse: 1.80997\n",
      "[1400]\ttraining's rmse: 1.86893\tvalid_1's rmse: 1.80623\n",
      "[1500]\ttraining's rmse: 1.86035\tvalid_1's rmse: 1.80185\n",
      "[1600]\ttraining's rmse: 1.85165\tvalid_1's rmse: 1.79796\n",
      "[1700]\ttraining's rmse: 1.84396\tvalid_1's rmse: 1.78859\n",
      "[1800]\ttraining's rmse: 1.83681\tvalid_1's rmse: 1.78533\n",
      "[1900]\ttraining's rmse: 1.82935\tvalid_1's rmse: 1.77858\n",
      "[2000]\ttraining's rmse: 1.8224\tvalid_1's rmse: 1.773\n",
      "[2100]\ttraining's rmse: 1.8156\tvalid_1's rmse: 1.76919\n",
      "[2200]\ttraining's rmse: 1.80942\tvalid_1's rmse: 1.76428\n",
      "[2300]\ttraining's rmse: 1.80333\tvalid_1's rmse: 1.76053\n",
      "[2400]\ttraining's rmse: 1.79775\tvalid_1's rmse: 1.75587\n",
      "[2500]\ttraining's rmse: 1.7919\tvalid_1's rmse: 1.74975\n",
      "[2600]\ttraining's rmse: 1.78638\tvalid_1's rmse: 1.74541\n",
      "[2700]\ttraining's rmse: 1.78116\tvalid_1's rmse: 1.74043\n",
      "[2800]\ttraining's rmse: 1.77579\tvalid_1's rmse: 1.73579\n",
      "[2900]\ttraining's rmse: 1.77085\tvalid_1's rmse: 1.73223\n",
      "[3000]\ttraining's rmse: 1.76568\tvalid_1's rmse: 1.7295\n",
      "[3100]\ttraining's rmse: 1.76096\tvalid_1's rmse: 1.7261\n",
      "[3200]\ttraining's rmse: 1.75616\tvalid_1's rmse: 1.72189\n",
      "[3300]\ttraining's rmse: 1.75157\tvalid_1's rmse: 1.71786\n",
      "[3400]\ttraining's rmse: 1.74761\tvalid_1's rmse: 1.71395\n",
      "[3500]\ttraining's rmse: 1.7431\tvalid_1's rmse: 1.71165\n",
      "[3600]\ttraining's rmse: 1.73883\tvalid_1's rmse: 1.7078\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.73883\tvalid_1's rmse: 1.7078\n",
      "Train CA_3...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.386484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5673\n",
      "[LightGBM] [Info] Number of data points in the train set: 4720563, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 3.50939\tvalid_1's rmse: 2.63475\n",
      "[200]\ttraining's rmse: 3.36208\tvalid_1's rmse: 2.56691\n",
      "[300]\ttraining's rmse: 3.26495\tvalid_1's rmse: 2.52561\n",
      "[400]\ttraining's rmse: 3.19273\tvalid_1's rmse: 2.47942\n",
      "[500]\ttraining's rmse: 3.14176\tvalid_1's rmse: 2.44954\n",
      "[600]\ttraining's rmse: 3.09375\tvalid_1's rmse: 2.41226\n",
      "[700]\ttraining's rmse: 3.05493\tvalid_1's rmse: 2.37876\n",
      "[800]\ttraining's rmse: 3.02457\tvalid_1's rmse: 2.363\n",
      "[900]\ttraining's rmse: 2.99785\tvalid_1's rmse: 2.34931\n",
      "[1000]\ttraining's rmse: 2.97014\tvalid_1's rmse: 2.34108\n",
      "[1100]\ttraining's rmse: 2.94679\tvalid_1's rmse: 2.33223\n",
      "[1200]\ttraining's rmse: 2.92231\tvalid_1's rmse: 2.31765\n",
      "[1300]\ttraining's rmse: 2.90125\tvalid_1's rmse: 2.30365\n",
      "[1400]\ttraining's rmse: 2.88204\tvalid_1's rmse: 2.29374\n",
      "[1500]\ttraining's rmse: 2.86493\tvalid_1's rmse: 2.28478\n",
      "[1600]\ttraining's rmse: 2.84803\tvalid_1's rmse: 2.2774\n",
      "[1700]\ttraining's rmse: 2.83123\tvalid_1's rmse: 2.26444\n",
      "[1800]\ttraining's rmse: 2.81613\tvalid_1's rmse: 2.25826\n",
      "[1900]\ttraining's rmse: 2.80199\tvalid_1's rmse: 2.2498\n",
      "[2000]\ttraining's rmse: 2.78808\tvalid_1's rmse: 2.24297\n",
      "[2100]\ttraining's rmse: 2.77404\tvalid_1's rmse: 2.23756\n",
      "Early stopping, best iteration is:\n",
      "[2056]\ttraining's rmse: 2.78004\tvalid_1's rmse: 2.23735\n",
      "Train CA_4...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.479932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5631\n",
      "[LightGBM] [Info] Number of data points in the train set: 4620050, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 1.53561\tvalid_1's rmse: 1.41825\n",
      "[200]\ttraining's rmse: 1.50157\tvalid_1's rmse: 1.40782\n",
      "[300]\ttraining's rmse: 1.47952\tvalid_1's rmse: 1.39284\n",
      "[400]\ttraining's rmse: 1.46465\tvalid_1's rmse: 1.38365\n",
      "[500]\ttraining's rmse: 1.45162\tvalid_1's rmse: 1.37645\n",
      "[600]\ttraining's rmse: 1.44036\tvalid_1's rmse: 1.36644\n",
      "[700]\ttraining's rmse: 1.43015\tvalid_1's rmse: 1.36214\n",
      "[800]\ttraining's rmse: 1.42121\tvalid_1's rmse: 1.35552\n",
      "[900]\ttraining's rmse: 1.41343\tvalid_1's rmse: 1.35068\n",
      "[1000]\ttraining's rmse: 1.40568\tvalid_1's rmse: 1.3456\n",
      "[1100]\ttraining's rmse: 1.39906\tvalid_1's rmse: 1.34047\n",
      "[1200]\ttraining's rmse: 1.39239\tvalid_1's rmse: 1.33793\n",
      "[1300]\ttraining's rmse: 1.38608\tvalid_1's rmse: 1.33442\n",
      "[1400]\ttraining's rmse: 1.38018\tvalid_1's rmse: 1.33089\n",
      "[1500]\ttraining's rmse: 1.3746\tvalid_1's rmse: 1.32726\n",
      "[1600]\ttraining's rmse: 1.36945\tvalid_1's rmse: 1.3249\n",
      "[1700]\ttraining's rmse: 1.36475\tvalid_1's rmse: 1.32163\n",
      "[1800]\ttraining's rmse: 1.3602\tvalid_1's rmse: 1.31877\n",
      "[1900]\ttraining's rmse: 1.35558\tvalid_1's rmse: 1.31604\n",
      "[2000]\ttraining's rmse: 1.35122\tvalid_1's rmse: 1.31306\n",
      "[2100]\ttraining's rmse: 1.34706\tvalid_1's rmse: 1.31054\n",
      "[2200]\ttraining's rmse: 1.3428\tvalid_1's rmse: 1.30836\n",
      "[2300]\ttraining's rmse: 1.3387\tvalid_1's rmse: 1.30611\n",
      "[2400]\ttraining's rmse: 1.33465\tvalid_1's rmse: 1.30275\n",
      "[2500]\ttraining's rmse: 1.3309\tvalid_1's rmse: 1.30004\n",
      "[2600]\ttraining's rmse: 1.32725\tvalid_1's rmse: 1.29741\n",
      "[2700]\ttraining's rmse: 1.3236\tvalid_1's rmse: 1.29248\n",
      "[2800]\ttraining's rmse: 1.32005\tvalid_1's rmse: 1.28892\n",
      "[2900]\ttraining's rmse: 1.31642\tvalid_1's rmse: 1.28618\n",
      "[3000]\ttraining's rmse: 1.31296\tvalid_1's rmse: 1.28378\n",
      "[3100]\ttraining's rmse: 1.30991\tvalid_1's rmse: 1.28111\n",
      "[3200]\ttraining's rmse: 1.30697\tvalid_1's rmse: 1.27864\n",
      "[3300]\ttraining's rmse: 1.3038\tvalid_1's rmse: 1.27523\n",
      "[3400]\ttraining's rmse: 1.3008\tvalid_1's rmse: 1.27094\n",
      "[3500]\ttraining's rmse: 1.29754\tvalid_1's rmse: 1.2685\n",
      "[3600]\ttraining's rmse: 1.29453\tvalid_1's rmse: 1.26587\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.29453\tvalid_1's rmse: 1.26587\n",
      "Train TX_1...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.407994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5689\n",
      "[LightGBM] [Info] Number of data points in the train set: 4761863, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.13683\tvalid_1's rmse: 1.79455\n",
      "[200]\ttraining's rmse: 2.05733\tvalid_1's rmse: 1.74394\n",
      "[300]\ttraining's rmse: 2.01114\tvalid_1's rmse: 1.70207\n",
      "[400]\ttraining's rmse: 1.97499\tvalid_1's rmse: 1.6708\n",
      "[500]\ttraining's rmse: 1.94585\tvalid_1's rmse: 1.64144\n",
      "[600]\ttraining's rmse: 1.92228\tvalid_1's rmse: 1.63083\n",
      "[700]\ttraining's rmse: 1.9028\tvalid_1's rmse: 1.6091\n",
      "[800]\ttraining's rmse: 1.88391\tvalid_1's rmse: 1.59781\n",
      "[900]\ttraining's rmse: 1.86827\tvalid_1's rmse: 1.59181\n",
      "[1000]\ttraining's rmse: 1.85204\tvalid_1's rmse: 1.58651\n",
      "[1100]\ttraining's rmse: 1.83849\tvalid_1's rmse: 1.57963\n",
      "[1200]\ttraining's rmse: 1.82575\tvalid_1's rmse: 1.56564\n",
      "[1300]\ttraining's rmse: 1.81382\tvalid_1's rmse: 1.55849\n",
      "[1400]\ttraining's rmse: 1.80277\tvalid_1's rmse: 1.54976\n",
      "[1500]\ttraining's rmse: 1.79099\tvalid_1's rmse: 1.54311\n",
      "[1600]\ttraining's rmse: 1.78086\tvalid_1's rmse: 1.53802\n",
      "[1700]\ttraining's rmse: 1.77118\tvalid_1's rmse: 1.53113\n",
      "[1800]\ttraining's rmse: 1.76265\tvalid_1's rmse: 1.52763\n",
      "[1900]\ttraining's rmse: 1.75454\tvalid_1's rmse: 1.51632\n",
      "[2000]\ttraining's rmse: 1.74636\tvalid_1's rmse: 1.51069\n",
      "[2100]\ttraining's rmse: 1.73845\tvalid_1's rmse: 1.50717\n",
      "[2200]\ttraining's rmse: 1.73068\tvalid_1's rmse: 1.50374\n",
      "[2300]\ttraining's rmse: 1.72403\tvalid_1's rmse: 1.49886\n",
      "[2400]\ttraining's rmse: 1.71681\tvalid_1's rmse: 1.49471\n",
      "[2500]\ttraining's rmse: 1.70934\tvalid_1's rmse: 1.4916\n",
      "[2600]\ttraining's rmse: 1.7029\tvalid_1's rmse: 1.48625\n",
      "[2700]\ttraining's rmse: 1.69701\tvalid_1's rmse: 1.48186\n",
      "[2800]\ttraining's rmse: 1.69069\tvalid_1's rmse: 1.47537\n",
      "[2900]\ttraining's rmse: 1.6847\tvalid_1's rmse: 1.47191\n",
      "[3000]\ttraining's rmse: 1.67878\tvalid_1's rmse: 1.46796\n",
      "[3100]\ttraining's rmse: 1.67323\tvalid_1's rmse: 1.46433\n",
      "[3200]\ttraining's rmse: 1.66817\tvalid_1's rmse: 1.46018\n",
      "[3300]\ttraining's rmse: 1.66228\tvalid_1's rmse: 1.45833\n",
      "[3400]\ttraining's rmse: 1.65744\tvalid_1's rmse: 1.45346\n",
      "[3500]\ttraining's rmse: 1.65238\tvalid_1's rmse: 1.44966\n",
      "[3600]\ttraining's rmse: 1.64699\tvalid_1's rmse: 1.44702\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.64699\tvalid_1's rmse: 1.44702\n",
      "Train TX_2...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.463588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5682\n",
      "[LightGBM] [Info] Number of data points in the train set: 4771138, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.5516\tvalid_1's rmse: 1.91614\n",
      "[200]\ttraining's rmse: 2.43971\tvalid_1's rmse: 1.87061\n",
      "[300]\ttraining's rmse: 2.37585\tvalid_1's rmse: 1.83758\n",
      "[400]\ttraining's rmse: 2.32442\tvalid_1's rmse: 1.81263\n",
      "[500]\ttraining's rmse: 2.28565\tvalid_1's rmse: 1.79435\n",
      "[600]\ttraining's rmse: 2.25461\tvalid_1's rmse: 1.77964\n",
      "[700]\ttraining's rmse: 2.22616\tvalid_1's rmse: 1.7688\n",
      "[800]\ttraining's rmse: 2.20414\tvalid_1's rmse: 1.75742\n",
      "[900]\ttraining's rmse: 2.18288\tvalid_1's rmse: 1.74497\n",
      "[1000]\ttraining's rmse: 2.16302\tvalid_1's rmse: 1.7288\n",
      "[1100]\ttraining's rmse: 2.14384\tvalid_1's rmse: 1.72327\n",
      "[1200]\ttraining's rmse: 2.12825\tvalid_1's rmse: 1.71786\n",
      "[1300]\ttraining's rmse: 2.11238\tvalid_1's rmse: 1.70935\n",
      "[1400]\ttraining's rmse: 2.09689\tvalid_1's rmse: 1.70276\n",
      "[1500]\ttraining's rmse: 2.08429\tvalid_1's rmse: 1.6992\n",
      "[1600]\ttraining's rmse: 2.07096\tvalid_1's rmse: 1.69158\n",
      "[1700]\ttraining's rmse: 2.05883\tvalid_1's rmse: 1.68644\n",
      "[1800]\ttraining's rmse: 2.04725\tvalid_1's rmse: 1.68211\n",
      "[1900]\ttraining's rmse: 2.03553\tvalid_1's rmse: 1.67333\n",
      "[2000]\ttraining's rmse: 2.02397\tvalid_1's rmse: 1.67024\n",
      "[2100]\ttraining's rmse: 2.01475\tvalid_1's rmse: 1.66519\n",
      "[2200]\ttraining's rmse: 2.00535\tvalid_1's rmse: 1.66121\n",
      "[2300]\ttraining's rmse: 1.99543\tvalid_1's rmse: 1.6559\n",
      "[2400]\ttraining's rmse: 1.98612\tvalid_1's rmse: 1.65059\n",
      "[2500]\ttraining's rmse: 1.9775\tvalid_1's rmse: 1.64621\n",
      "[2600]\ttraining's rmse: 1.96949\tvalid_1's rmse: 1.64273\n",
      "[2700]\ttraining's rmse: 1.96108\tvalid_1's rmse: 1.64003\n",
      "[2800]\ttraining's rmse: 1.95364\tvalid_1's rmse: 1.63611\n",
      "[2900]\ttraining's rmse: 1.94622\tvalid_1's rmse: 1.63364\n",
      "[3000]\ttraining's rmse: 1.93843\tvalid_1's rmse: 1.62856\n",
      "[3100]\ttraining's rmse: 1.93174\tvalid_1's rmse: 1.62185\n",
      "[3200]\ttraining's rmse: 1.92523\tvalid_1's rmse: 1.61837\n",
      "[3300]\ttraining's rmse: 1.91785\tvalid_1's rmse: 1.61469\n",
      "[3400]\ttraining's rmse: 1.91081\tvalid_1's rmse: 1.60922\n",
      "[3500]\ttraining's rmse: 1.90411\tvalid_1's rmse: 1.60581\n",
      "[3600]\ttraining's rmse: 1.89751\tvalid_1's rmse: 1.60152\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.89751\tvalid_1's rmse: 1.60152\n",
      "Train TX_3...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.596024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5680\n",
      "[LightGBM] [Info] Number of data points in the train set: 4702580, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 2.21241\tvalid_1's rmse: 2.04669\n",
      "[200]\ttraining's rmse: 2.11833\tvalid_1's rmse: 1.96322\n",
      "[300]\ttraining's rmse: 2.06221\tvalid_1's rmse: 1.92247\n",
      "[400]\ttraining's rmse: 2.02189\tvalid_1's rmse: 1.90481\n",
      "[500]\ttraining's rmse: 1.99017\tvalid_1's rmse: 1.8684\n",
      "[600]\ttraining's rmse: 1.96331\tvalid_1's rmse: 1.84516\n",
      "[700]\ttraining's rmse: 1.9401\tvalid_1's rmse: 1.83347\n",
      "[800]\ttraining's rmse: 1.92016\tvalid_1's rmse: 1.82222\n",
      "[900]\ttraining's rmse: 1.9027\tvalid_1's rmse: 1.80753\n",
      "[1000]\ttraining's rmse: 1.88762\tvalid_1's rmse: 1.79617\n",
      "[1100]\ttraining's rmse: 1.87345\tvalid_1's rmse: 1.78603\n",
      "[1200]\ttraining's rmse: 1.85891\tvalid_1's rmse: 1.77585\n",
      "[1300]\ttraining's rmse: 1.84572\tvalid_1's rmse: 1.76684\n",
      "[1400]\ttraining's rmse: 1.83369\tvalid_1's rmse: 1.76019\n",
      "[1500]\ttraining's rmse: 1.82269\tvalid_1's rmse: 1.7496\n",
      "[1600]\ttraining's rmse: 1.81292\tvalid_1's rmse: 1.74038\n",
      "[1700]\ttraining's rmse: 1.80306\tvalid_1's rmse: 1.72882\n",
      "[1800]\ttraining's rmse: 1.79337\tvalid_1's rmse: 1.71973\n",
      "[1900]\ttraining's rmse: 1.78502\tvalid_1's rmse: 1.71052\n",
      "[2000]\ttraining's rmse: 1.7757\tvalid_1's rmse: 1.70282\n",
      "[2100]\ttraining's rmse: 1.76791\tvalid_1's rmse: 1.69779\n",
      "[2200]\ttraining's rmse: 1.76029\tvalid_1's rmse: 1.69105\n",
      "[2300]\ttraining's rmse: 1.75266\tvalid_1's rmse: 1.68541\n",
      "[2400]\ttraining's rmse: 1.74521\tvalid_1's rmse: 1.67983\n",
      "[2500]\ttraining's rmse: 1.73798\tvalid_1's rmse: 1.66957\n",
      "[2600]\ttraining's rmse: 1.73142\tvalid_1's rmse: 1.66385\n",
      "[2700]\ttraining's rmse: 1.72508\tvalid_1's rmse: 1.65681\n",
      "[2800]\ttraining's rmse: 1.71869\tvalid_1's rmse: 1.65026\n",
      "[2900]\ttraining's rmse: 1.7126\tvalid_1's rmse: 1.64392\n",
      "[3000]\ttraining's rmse: 1.70647\tvalid_1's rmse: 1.63869\n",
      "[3100]\ttraining's rmse: 1.70093\tvalid_1's rmse: 1.63236\n",
      "[3200]\ttraining's rmse: 1.69493\tvalid_1's rmse: 1.62846\n",
      "[3300]\ttraining's rmse: 1.68912\tvalid_1's rmse: 1.62435\n",
      "[3400]\ttraining's rmse: 1.68337\tvalid_1's rmse: 1.62013\n",
      "[3500]\ttraining's rmse: 1.6781\tvalid_1's rmse: 1.61673\n",
      "[3600]\ttraining's rmse: 1.67325\tvalid_1's rmse: 1.61371\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3600]\ttraining's rmse: 1.67325\tvalid_1's rmse: 1.61371\n",
      "Train WI_1...\n",
      "[LightGBM] [Info] Saving data to binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Info] Load from binary file input\\bottom_out\\train_data.bin\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.572059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5653\n",
      "[LightGBM] [Info] Number of data points in the train set: 4532270, number of used features: 29\n",
      "[LightGBM] [Warning] Using self-defined objective function\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 1.76925\tvalid_1's rmse: 1.69106\n",
      "[200]\ttraining's rmse: 1.71869\tvalid_1's rmse: 1.6614\n",
      "[300]\ttraining's rmse: 1.68772\tvalid_1's rmse: 1.63912\n",
      "[400]\ttraining's rmse: 1.66583\tvalid_1's rmse: 1.62217\n",
      "[500]\ttraining's rmse: 1.64849\tvalid_1's rmse: 1.60933\n",
      "[600]\ttraining's rmse: 1.63456\tvalid_1's rmse: 1.60022\n",
      "[700]\ttraining's rmse: 1.6229\tvalid_1's rmse: 1.59133\n",
      "[800]\ttraining's rmse: 1.61055\tvalid_1's rmse: 1.5833\n"
     ]
    }
   ],
   "source": [
    "########################### Train Models\n",
    "#################################################################################\n",
    "for store_id in STORES_IDS:\n",
    "    print('Train {}...'.format(store_id))\n",
    "    \n",
    "    # Get grid for current store: 1min\n",
    "    # features_columns is ['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', \n",
    "    # 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', \n",
    "    # 'price_min', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', \n",
    "    # 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', \n",
    "    # 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end']\n",
    "    grid_df, features_columns = get_data_by_store(store_id)\n",
    "    \n",
    "    # Masks for \n",
    "    # Train (All data less than 1913)\n",
    "    # \"Validation\" (Last 28 days - not real validatio set)\n",
    "    # Test (All data greater than 1913 day, \n",
    "    #       with some gap for recursive features)\n",
    "    train_mask = grid_df['d'] <= END_TRAIN\n",
    "    valid_mask = train_mask & (grid_df['d'] > (END_TRAIN - P_HORIZON))\n",
    "    preds_mask = grid_df['d'] > (END_TRAIN - 100)\n",
    "    \n",
    "    # Apply masks and save lgb dataset as bin\n",
    "    # to reduce memory spikes during dtype convertations\n",
    "    # https://github.com/Microsoft/LightGBM/issues/1032\n",
    "    # \"To avoid any conversions, you should always use np.float32\"\n",
    "    # or save to bin before start training\n",
    "    # https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/53773\n",
    "    train_data = lgb.Dataset(grid_df[train_mask][features_columns], \n",
    "                       label=grid_df[train_mask][TARGET])\n",
    "    train_data.save_binary(output_parent/'train_data.bin')\n",
    "    train_data = lgb.Dataset(output_parent/'train_data.bin')\n",
    "    \n",
    "    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], \n",
    "                       label=grid_df[valid_mask][TARGET])\n",
    "    \n",
    "    # Saving part of the dataset for later predictions\n",
    "    # Removing features that we need to calculate recursively \n",
    "    grid_df = grid_df[preds_mask].reset_index(drop=True)\n",
    "    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]\n",
    "    grid_df = grid_df[keep_cols]\n",
    "    grid_df.to_pickle(output_parent/f'test_{store_id}.pkl')\n",
    "    del grid_df\n",
    "    \n",
    "    # Launch seeder again to make lgb training 100% deterministic\n",
    "    # with each \"code line\" np.random \"evolves\" \n",
    "    # so we need (may want) to \"reset\" it\n",
    "    seed_everything(SEED)\n",
    "    estimator = lgb.train(lgb_params,\n",
    "                          train_data,\n",
    "                          num_boost_round = 3600, # =n_estimators = num_trees\n",
    "                          early_stopping_rounds = 50, \n",
    "                          valid_sets = [train_data, valid_data],\n",
    "                          verbose_eval = 100,\n",
    "                          fobj = custom_asymmetric_train\n",
    "                          )\n",
    "    \n",
    "    # Save model - it's not real '.bin' but a pickle file\n",
    "    # estimator = lgb.Booster(model_file='model.txt')\n",
    "    # can only predict with the best iteration (or the saving iteration)\n",
    "    # pickle.dump gives us more flexibility\n",
    "    # like estimator.predict(TEST, num_iteration=100)\n",
    "    # num_iteration - number of iteration want to predict with, \n",
    "    # NULL or <= 0 means use best iteration\n",
    "    model_name = f'lgb_model_{store_id}_v{VER}.bin'\n",
    "    pickle.dump(estimator, open(output_parent/model_name, 'wb'))\n",
    "\n",
    "    # Remove temporary files and objects \n",
    "    # to free some hdd space and ram memory\n",
    "    os.remove(output_parent/\"train_data.bin\")\n",
    "    del train_data, valid_data, estimator\n",
    "    gc.collect()\n",
    "    \n",
    "    # \"Keep\" models features for predictions\n",
    "    MODEL_FEATURES = features_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Predict\n",
    "#################################################################################\n",
    "\n",
    "# Create Dummy DataFrame to store predictions\n",
    "all_preds = pd.DataFrame()\n",
    "\n",
    "# Join back the Test dataset with \n",
    "# a small part of the training data \n",
    "# to make recursive features\n",
    "base_test = get_base_test()\n",
    "print(\"orginal base_test by combining all test_storeid.pkl\")\n",
    "display(base_test)\n",
    "\n",
    "# Timer to measure predictions time \n",
    "main_time = time.time()\n",
    "\n",
    "# Loop over each prediction day\n",
    "# As rolling lags are the most timeconsuming\n",
    "# we will calculate it for whole day\n",
    "for PREDICT_DAY in range(1,29):    \n",
    "    print('Predict | Day:', PREDICT_DAY)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Make temporary grid to calculate rolling lags\n",
    "    grid_df = base_test.copy()\n",
    "        \n",
    "    for store_id in STORES_IDS:\n",
    "        \n",
    "        # Read all our models and make predictions\n",
    "        # for each day/store pairs\n",
    "        model_name = f'lgb_model_{store_id+}_v{VER}.bin' \n",
    "        if USE_AUX: # use pretrained models \n",
    "            model_name = AUX_MODEL + model_name\n",
    "        \n",
    "        estimator = pickle.load(open(output_parent/model_name, 'rb'))\n",
    "        \n",
    "        day_mask = base_test['d']==(END_TRAIN + PREDICT_DAY)\n",
    "        store_mask = base_test['store_id']==store_id\n",
    "        \n",
    "        mask = (day_mask) & (store_mask)\n",
    "        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])\n",
    "    \n",
    "    # Make good column naming and add \n",
    "    # to all_preds DataFrame\n",
    "    temp_df = base_test[day_mask][['id',TARGET]]\n",
    "    temp_df.columns = ['id', f'F{PREDICT_DAY}']\n",
    "    if 'id' in list(all_preds):\n",
    "        all_preds = all_preds.merge(temp_df, on=['id'], how='left')\n",
    "    else:\n",
    "        all_preds = temp_df.copy()\n",
    "        \n",
    "    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),\n",
    "                  ' %0.2f min total |' % ((time.time() - main_time) / 60),\n",
    "                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))\n",
    "    del temp_df\n",
    "    \n",
    "all_preds = all_preds.reset_index(drop=True)\n",
    "print(\"After collection of predictions per stores and per predict_day:\")\n",
    "display(all_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Export\n",
    "#################################################################################\n",
    "# Reading competition sample submission and\n",
    "# merging our predictions\n",
    "# As we have predictions only for \"_validation\" data\n",
    "# we need to do fillna() for \"_evaluation\" items\n",
    "submission = pd.read_csv(ORIGINAL+'sample_submission.csv')[['id']]\n",
    "submission = submission.merge(all_preds, on=['id'], how='left').fillna(0)\n",
    "submission.to_csv(output_parent/'submission_v{VER}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('poetry-modeling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9fdaeeb7f23b80181d796b81c382cf57b09494118cd09f4f1f2649b466761a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
